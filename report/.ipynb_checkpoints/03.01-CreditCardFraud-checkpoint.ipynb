{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11ba5abc",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5c2571",
   "metadata": {},
   "source": [
    "## Context & Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66b385",
   "metadata": {},
   "source": [
    "We are investigating credit card fraud and whether it is possible to detect this fraud from given data. <br>\n",
    "\n",
    "We have credit card transaction records as structured data consisting of a mix of quantitative and categorical variables including transaction amount, merchant, time and location. Important categorical variables also include transaction type (online or in-store) and customer profile (regular, irregular). This structured data comes in the form of spreadsheets and can be analysed using Python Pandas as a dataframe. This data could be imported as a CSV.\n",
    "\n",
    "There is also unstructured data available from customer complaints and enquiries relating to credit card fraud. This may not seem relevant for our initial statistical analysis but will provide excellent context for our situation. We could use ChatGPT to investigate large amounts of customer complaints and pick out key words to potentially indicate what we should look for in our analysis.\n",
    "Some of the available datasets have anonymised the variable name due to confidentiality of the data. We must trust that these variables are relevant in contributing to the analysis of credit card fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0dbd5",
   "metadata": {},
   "source": [
    "\n",
    "## Packages, Resources & GitHub\n",
    "There are many resources available including Kaggle for datasets, Python libraries including Pandas and NumPy for data analysis and scikit-learn for machine learning models in our later investigation. Also, there is a large amount of documentation explaining fraud detection algorithms which can be leveraged in our scenario. Furthermore, there are many GitHub repositories investigating this scenario. We could use some of the analyses in these and either expand on them or use these to help us to design and implement our models.\n",
    "\n",
    "We aim to create a successful fraud detection algorithm and we must use our resources effectively and efficiently to do so. The Python packages will be essential for our initial data analysis and then when creating our algorithm. Machine learning algorithms including k-means clustering and logistic regression will likely be helpful for classifying transactions as fraudulent.\n",
    "\n",
    "GitHub allows us to collaborate remotely as a group which is vital for the project due to the time constraints. Privacy issues are less significant since the data that we will be using is publicly available. Significant issues may occur if GitHub itself shuts down and we cannot access our repository to work together on the project.\n",
    "\n",
    "We will perform exploratory data analysis on a particular dataset, using Pandas and NumPy to analyse the data, with Matplotlib and Seaborn to create interesting visualisations.\n",
    "\n",
    "The dataset that we have chosen to analyse is called [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data), and was found on [Kaggle](https://www.kaggle.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813795c3",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f576a1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6aab35f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'creditcard.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# load the dataset using pandas\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreditcard.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# quick data preview\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'creditcard.csv'"
     ]
    }
   ],
   "source": [
    "# load the dataset using pandas\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "# quick data preview\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51c2e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape of dataset\n",
    "print(f\"Rows: {df.shape[0]}.\")\n",
    "print(f\"Columns: {df.shape[1]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc31eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# basic info to understand data types and missing values\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8474a297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for any missing values\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78d7b11",
   "metadata": {},
   "source": [
    "There are no null values in the dataset. Thus, we do not need to clean or pre-process null values out of the dataset and we can start looking directly at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5b9c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01cd0fd",
   "metadata": {},
   "source": [
    "We can see that 'Amount' is very variable due to the high standard deviation and also very skewed since the mean is higher than the third quartile. The 'Class' column is also very skewed with many more non-fraudulent cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc8232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot fraud vs non-fraud counts\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.countplot(x='Class', data=df, palette='Set1')\n",
    "plt.title('Fraud (1) vs Non-Fraud (0)')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7234bb",
   "metadata": {},
   "source": [
    "Clearly, the data is very skewed and there are many more cases of 'non-fraud' than 'fraud'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888d5d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of transaction amounts by class\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(df[df['Class'] == 0]['Amount'], bins=40, color='blue', kde=True, label='Non-Fraud', alpha=0.6)\n",
    "sns.histplot(df[df['Class'] == 1]['Amount'], bins=40, color='red', kde=True, label='Fraud', alpha=0.6)\n",
    "plt.title('Transaction Amount Distribution by Class')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc95d76f",
   "metadata": {},
   "source": [
    "Non-fraudulent cases typically have lower 'Amount' values and fraudulent cases typically have a wider range of 'Amount' values with some very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of feature correlations\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm_r')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac65225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of feature correlations, focusing on correlation with 'Class'\n",
    "plt.figure(figsize=(12, 8))\n",
    "corr_matrix = df.corr()\n",
    "sns.heatmap(corr_matrix[['Class']], annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "plt.title('Correlation with Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb62c8b",
   "metadata": {},
   "source": [
    "Correlations are generally very low, suggesting there may be a non-linear relationship with 'Class'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9123df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier detection for transaction amounts\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x=df['Amount'], color='lightblue')\n",
    "plt.title('Transaction Amount Outliers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c32b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outlier detection for transaction amounts by class\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.boxplot(x='Class', y='Amount', data=df, palette='Set1')\n",
    "plt.title('Transaction Amount Outliers by Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482051a2",
   "metadata": {},
   "source": [
    "Most transaction amounts are very small but there are a number of very high transactions, surprisingly more outliers from the non-fraudulent class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01f6bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairplot for top 5 most correlated predictors\n",
    "top_predictors = corr_matrix['Class'].abs().sort_values(ascending=False).index[1:6]  # top 5 predictors\n",
    "\n",
    "sample = df.sample(500)  # sample 500 rows to speed up visualization\n",
    "sns.pairplot(sample[top_predictors.union(['Class'])], hue='Class', palette='cool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afa67bf",
   "metadata": {},
   "source": [
    "The pair plots show that there may be non-linear relationship between the predictors since there are clusters of points. We may need to consider clustering methods to further analyse these relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4edde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skewness of transaction amounts\n",
    "print(\"Skewness in 'Amount':\", df['Amount'].skew())\n",
    "\n",
    "# skewness of transaction amounts by class\n",
    "skew_non_fraud = df[df['Class'] == 0]['Amount'].skew()\n",
    "skew_fraud = df[df['Class'] == 1]['Amount'].skew()\n",
    "print(f\"Skewness in 'Amount' for non-fraud: {skew_non_fraud}\")\n",
    "print(f\"Skewness in 'Amount' for fraud: {skew_fraud}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86c5648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count fraud vs non-fraud transactions\n",
    "fraud_cases = df[df['Class'] == 1].shape[0]\n",
    "non_fraud_cases = df[df['Class'] == 0].shape[0]\n",
    "\n",
    "print(f\"Number of fraud cases: {fraud_cases}\")\n",
    "print(f\"Number of non-fraud cases: {non_fraud_cases}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a11753f",
   "metadata": {},
   "source": [
    "Again, we can see that the data is very skewed and there are many more cases of 'non-fraud' than 'fraud'. The non-fraudulent transaction values ('Amount') have much more skewness than the fraudlent. We may need to transform the data to analyse it more successfully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fa7b5a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We can see that the data is highly skewed towards non-fraudulent cases and we must implement methods to overcome this if we want to train a model to identify fraud. We must use the vast range of named resources to learn more about such methods and models. One way of dealing with imbalanced data is explored in the next document.\n",
    "\n",
    "This is an initial analysis of the data which we will expand on in the next project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
