{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dda0eba4-d8ee-408e-8652-d5651a9d9ee3",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection\n",
    "\n",
    "## Dataset\n",
    "This document will further explore the dataset introduced on the last document, namely [Credit Card Fraud Detection](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud/data), from [Kaggle](https://www.kaggle.com), but will focus on one way of dealing with imbalanced data to better train predictive models.\n",
    "\n",
    "First we will import the modules that we will need to analyse our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f559c8-d867-4942-8d22-41fffe1b49a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ccd08-96de-4c0a-86c7-53d7e2a13f21",
   "metadata": {},
   "source": [
    "Next we will read the dataset, which unfortunately must be downloaded manually. I attempted to upload the .csv file directly to the github repository but was not able to due to the file size being greater than 25mb, so unfortunately for now, to run this document, the dataset will need to be downloaded manually and placed in the same directory as the .ipynb file.\n",
    "\n",
    "To read the csv file, we will use the pandas module that we previously imported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcefcd16-60d3-430a-a164-e9529b8bc198",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'creditcard.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcreditcard.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mloc[:,[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTime\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV1\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mV28\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mhead() \u001b[38;5;66;03m# We exclude V2-V27 to make it more readable\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'creditcard.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('creditcard.csv')\n",
    "df.loc[:,['Time','V1','V28','Amount','Class']].head() # We exclude V2-V27 to make it more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26940f6-1170-4f45-8d3c-2368a2348b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453f5d2e-9095-462b-8a50-4731e679035a",
   "metadata": {},
   "source": [
    "We see that the dataset consists of 31 columns, or features, each with 284,807 entries. The features are as follows, taken from the source of the dataset, Kaggle.\n",
    "\n",
    "| Column Name   | Feature Description                                 |\n",
    "|---------------|-----------------------------------------------------|\n",
    "| Time          | The seconds elapsed since the first transaction in the dataset and this one |\n",
    "| V1-V28        | These are the principal components, anonymised for confidentiality |\n",
    "| Amount        | The transaction amount                              |\n",
    "| Class         | The response variable, 0 for normal, 1 for fraud    |\n",
    "\n",
    "Next we'll take a closer look at the 3 variables that are named."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bb512b-2393-42da-b987-440fb9134841",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"Time\",\"Amount\",\"Class\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24125fe8-6d4e-4f83-8a2e-e286250fe5a8",
   "metadata": {},
   "source": [
    "From this we see that both **Time** and **Amount** vary a fair amount, and the summary for **Class** doesn't seem very useful. This makes sense as we know that it only takes 2 values. The very low mean does suggest that the data is heavily skewed, we will look more into this. The data for **Time** also suggests that there were periods where more transactions came through. Let's plot these to get a better look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8aa8e-d30e-4849-850a-be26539640ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5,10))\n",
    "\n",
    "ax1, ax2, ax3 = fig.subplots(3)\n",
    "amount_vals = df['Amount'].values\n",
    "time_vals = df['Time'].values\n",
    "class_vals = df['Class'].values\n",
    "\n",
    "ax1.plot(amount_vals)\n",
    "ax1.set_title('Amount Of Money Transacted')\n",
    "ax1.xlabel('Index Of Data')\n",
    "ax1.ylabel('Amount')\n",
    "\n",
    "ax2.plot(time_vals)\n",
    "ax2.set_title('Time Since Initial Transaction')\n",
    "ax2.xlabel('Index Of Data')\n",
    "ax2.ylabel('Time')\n",
    "\n",
    "ax3.hist(class_vals,bins = 2)\n",
    "ax3.set_title('Fraud Response, 0 = Normal, 1 = Fraud')\n",
    "ax3.xlabel('Value, 0 or 1')\n",
    "ax3.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd93e52-35cf-4ed4-8e1d-3601d2d867b6",
   "metadata": {},
   "source": [
    "From the plot for **Amount**, we see that the majority of the transactions were a fairly low amount of money. This is supported by the mean gathered previously, which was 88.35 to 2 decimal places.\n",
    "\n",
    "Next, the plot for **Time** tells us that there were indeed periods of increased activity, whilst the rest was roughly linear. From inspection of the graph, these periods of increased activity occur at around 10,000 and then 150,000 seconds.\n",
    "\n",
    "Finally, from the plot for **Class**, we see that the data is in fact heavily skewed, with the bar for the fraudulent cases barely visible, if at all. If we take a look back to the mean gathered before, which was $0.001727$, this shows that only 0.1727% of the transactions have been identified as fraudulent. We can check this more rigorously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e80b94-9ca6-4d38-acc8-958a1efb99cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fraudcount = len([x for x in df['Class'] if x == 1])\n",
    "totalcount = (df.shape)[0]\n",
    "print('There are {} fraudulent cases, so out of the {} total cases, that gives {:.4f}% of the cases as fraudulent.'\n",
    "      .format(fraudcount,totalcount,100*fraudcount/totalcount))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edecf43-e958-4af9-9d08-1e5cb0bf3e9d",
   "metadata": {},
   "source": [
    "Having data this skewed can make it difficult to train predictive models since there are so few fraudulent cases to train them on. One way of dealing with this is explored in the Kaggle notebook called **'Credit Fraud || Dealing with Imbalanced Datasets'** by Janio Martinez Bachmann, found [here](https://www.kaggle.com/code/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets#notebook-container). The way that Bachmann deals with the imbalance in the data, is by taking a random subsample of the non-fraudulent cases of the same size as the fraudulent cases, i.e. 492 non-fraudulent cases, compared against the 492 fraudulent cases. Firstly, however, he scales the data in the **Time** and **Amount** features, in a similar way that the **V1-V28** features have been scaled.\n",
    "\n",
    "From here on, all comments in the coding are my own, whilst the code itself is taken from Bachmann's Kaggle code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95036e5d-aa15-4ebb-9fd7-37d8507c110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First he imports this package, one that allows for easy scaling of data. It scales the data to have mean 0 and variance 1.\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rob_scaler = RobustScaler()\n",
    "\n",
    "# These next lines add the scaled data onto the end of the dataframe.\n",
    "df['scaled_amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['scaled_time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "# The unscaled data is then dropped.\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "\n",
    "scaled_amount = df['scaled_amount']\n",
    "scaled_time = df['scaled_time']\n",
    "\n",
    "# And then replaced by moving the scaled data to the front of the dataframe.\n",
    "df.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\n",
    "df.insert(0, 'scaled_amount', scaled_amount)\n",
    "df.insert(1, 'scaled_time', scaled_time)\n",
    "\n",
    "# The data is now scaled\n",
    "df.loc[:,['scaled_amount','scaled_time','V1','V28','Class']].head() # Where we again exclude V2-V27 to make things more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5686c58e-58ce-404c-906e-18928a61a71b",
   "metadata": {},
   "source": [
    "Now that the data is scaled, he creates the subsample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ba70e-3f3d-481f-81e5-dc44e2a9f101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First he randomly shuffles the data.\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "# Then he isolated the fraudulent data, and creates the non-fraud subsample, then concatinates them.\n",
    "fraud_df = df.loc[df['Class'] == 1]\n",
    "non_fraud_df = df.loc[df['Class'] == 0][:492]\n",
    "\n",
    "normal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n",
    "\n",
    "# Then once again shuffles the data.\n",
    "new_df = normal_distributed_df.sample(frac=1, random_state=42)\n",
    "\n",
    "new_df.loc[:,['scaled_amount','scaled_time','V1','V28','Class']].head() # Where we again exclude V2-V27 to make things more readable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758b6211-a5ac-4455-8573-a07070022540",
   "metadata": {},
   "source": [
    "With this new subsample, Bachmann runs a correlation matrix to hopefully get an idea of any correlation between the **Class** (i.e. fraudulent or not) and any of the other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ace1d-8bbd-470c-a4d8-1401c39a0ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the figure.\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n",
    "\n",
    "# This plot is a correlation matrix for the entire dataframe, before he had gathered the subsample.\n",
    "corr = df.corr()\n",
    "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax1)\n",
    "ax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n",
    "\n",
    "# This plot is a correlation matrix for the subsample.\n",
    "sub_sample_corr = new_df.corr()\n",
    "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':20}, ax=ax2)\n",
    "ax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c257f-a026-41ef-8d8a-e719c719614f",
   "metadata": {},
   "source": [
    "These correlation matrices are strikingly different, and it is clear that creating the subsample was incredibly important for this dataset. The whole dataframe in the first plot does not show any clear correlation. This is because training on an overabundance of non-fraudulent cases makes the method expect that most cases are fraudulent, whilst training on the equal distribution subsample trains it to equally detect fraudulent and non-fraudulent.\n",
    "\n",
    "From Bachmann's own investigation of this second correlation matrix, he concludes that **V17, V14, V12, and V10** all show clear negative correlation, whilst **V2, V4, V11, and V19** all show clear positive correlation. This can be seen clearly by inspecting the final row or column, which shows the correlation between the **Class** feature and the other features.\n",
    "\n",
    "Whilst Bachmann's analysis goes much further than this, this notebook will stop here. I think that this brief analysis shows the significance of understanding and manipulating the data available to improve the models that you apply to it, you cannot merely go in blindly and hope that the model will do all the work for you, as this will not give any usable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
